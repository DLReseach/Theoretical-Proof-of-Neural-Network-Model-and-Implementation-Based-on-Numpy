# 矩阵形式的线性回归模型

## 矩阵形式的线性回归模型公式表和符号表

矩阵形式的线性回归模型计算公式如下，后文对这些公式给出了详细的证明过程。

### 公式表
|模型名称|模型输出函数|模型损失函数|模型损失函数的梯度|
|-|-|-|-|
线性回归模型|$A=X\theta$|$Loss=\frac{1}{m}{\Vert A^L-Y \Vert}^2$|$\nabla_{\boldsymbol{\theta}} Loss= \dfrac{2}{m} {X}^T (A - Y)$|

### 符号表
|符号|向量形式|维度|意义
|-|-|-|-|
|$x$|$[x_0,x_1,...,x_n]^T$|(n,1)|$x$ 表示一个样本实例， $x_i$ 表示向量的第 $i$ 维特征。|
|$X$|$[{(x^{(1)})}^T,{(x^{(2)})}^T,...{(x^{(m)})}^T]$| (m,n)|样本矩阵，一共 $m$ 个样例，每个样例有 $n$ 个特征值，$x^{(i)}$ 表示第 $i$ 个样例。|
|$\theta$|${[\theta_0,\theta_1,..,\theta_n]}^T$|(n,1)|权重向量|
|$A$|${[{\theta}^Tx^{(1)},{\theta}^Tx^{(2)},...,{\theta}^Tx^{(m)}]}^T$|(m,1)|模型预测回归值向量|
|$Y$|${[y^{(1)},y^{(2)},...,y^{(m)}]}^T$|(m,1)|回归值向量|
|$Loss$|标量|(1,1)|损失函数|
|$\nabla_{\boldsymbol{\theta}} Loss$||(n,m)|损失函数梯度矩阵|

## 线性回归模型的输出函数

$\hat{y}=\theta_0+\theta_1x_1+\theta_2x_2+···+\theta_nx_n$

|符号|意义|
|-|-|-|-|
|$\hat{y}$|输出函数值|
|$n$|特征个数|
|$x_i$|样本向量 $x$ 的第 $i$ 个特征值|
|$\theta_i$|参数向量 $\theta$ 的 $i$ 个权重值|

线性回归模型输出函数的向量形式，

$\hat{y}=h_{\theta}(x)={\theta}^T·x$

|符号|名称|维度|
|-|-|-|-|
|$h_{\theta}(x)$|假设函数|
|$\theta$|参数向量|(n,1)|
|$x$|样例向量|(n,1)|

## 线性回归模型的均方差损失函数

$$
\text{MSE}(\mathbf{X}, h_{\boldsymbol{\theta}}) = \dfrac{1}{m} \sum\limits_{i=1}^{m}{(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})^2}
$$

线性回归模型均方差MSE损失函数的向量形式，

$$Loss=\frac{1}{m}{(X\theta-Y)}^T(X\theta-Y)$$

|符号|向量形式|维度|意义
|-|-|-|-|
|$Loss$|标量|(1,1)|损失函数|
|$x$|$[x_0,x_1,...,x_n]^T$|(n,1)|$x$ 表示一个样本实例， $x_i$ 表示向量的第 $i$ 维特征。|
|$\theta$|${[\theta_0,\theta_1,..,\theta_n]}^T$|(n,1)|权重向量|
|$X$|$[{(x^{(1)})}^T,{(x^{(2)})}^T,...{(x^{(m)})}^T]$| (m,n)|样本矩阵，一共 $m$ 个样例，每个样例有 $n$ 个特征值，$x^{(i)}$ 表示第 $i$ 个样例。|
|${y}^{(i)}$|标量|(1,1)|回归值|
|$Y$|${[y^{(1)},y^{(2)},...,y^{(m)}]}^T$|(m,1)|回归值向量|

## 均方差损失函数的梯度

均方差损失函数的定义为，
$$
Loss= \dfrac{1}{m} \sum\limits_{i=1}^{m}{(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})^2}
$$

均方差损失函数的梯度计算公式的一般形式为：

$$
\dfrac{\partial Loss}{\partial \theta_j} = \dfrac{2}{m}\sum\limits_{i=1}^{m}(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})\, x_j^{(i)}
$$

均方差损失函数的梯度计算公式的向量形式为：

$$
\dfrac{\partial Loss}{\partial \theta} = \dfrac{2}{m}\sum\limits_{i=1}^{m}(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})\, x^{(i)}
$$

均方差损失函数的梯度计算公式的矩阵形式为：

$$
\nabla_{\boldsymbol{\theta}} Loss
 = \dfrac{2}{m} \mathbf{X}^T (\mathbf{X} \boldsymbol{\theta} - \mathbf{Y})
$$

若令线性回归模型输出矩阵为 $A$, $A=X\theta$，那么上面的式子还可以简化为，

$$
\nabla_{\boldsymbol{\theta}} Loss
 = \dfrac{2}{m} {X}^T (A - Y)
$$

矩阵形式均方差损失函数的梯度向量计算公式，其实更能揭示均方差损失函数权重矩阵梯度的本质意义，$A-Y$ 就是模型预测与实际标签的差值，这里可以定义为 $Eerror=A-Y$，正是这些差值驱动着模型不断的调整。很显然，当 $A$ 与 $Y$ 基本相等时， 差值 $Eerror \approx 0$, 那么 $\nabla_{w} Loss = \frac{1}{m}{X}^TError \approx0 $，交叉熵损失函数权重矩阵梯度约等于 0，模型训练完成。

## 证明：矩阵形式均方差损失函数的梯度向量计算公式

均方差损失函数的梯度计算公式的向量形式为：

$$
\dfrac{\partial Loss}{\partial \theta} = \dfrac{2}{m}\sum\limits_{i=1}^{m}(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})\, x^{(i)}
$$

下面我们对均方差损失函数的梯度计算公式的向量形式做变形，

$$
\begin{aligned}
\dfrac{\partial Loss}{\partial \theta} &= \frac{2}{m} \sum_{i=1}^m \nabla_{\theta} {loss}^{(i)} \\
  &= \frac{2}{m} (\nabla_{\theta} {loss}^{(1)} + \nabla_{\theta} {loss}^{(2)} + ··· + \nabla_{\theta} {loss}^{(m)} )\\
&= \frac{2}{m} ( x^{(1)}({\theta}^Tx^{(1)}-y^1) +x^{(2)}({\theta}^Tx^{(2)}-y^2)+ ··· + x^{(m)}({\theta}^Tx^{(m)}-y^m)\\
&= \frac{2}{m}{X}^T(X\theta-{Y}) \\
\end{aligned}
$$

即，均方差损失函数的梯度计算公式的矩阵形式为：

$$
\nabla_{\boldsymbol{\theta}} Loss
 = \dfrac{2}{m} \mathbf{X}^T (\mathbf{X} \boldsymbol{\theta} - \mathbf{Y})
$$

令 $A=X\theta$，

则，

$$
\nabla_{\boldsymbol{\theta}} Loss
 = \dfrac{2}{m} {X}^T (A - Y)
$$

## 线性模型的正则化

对于一个线性模型，正则化典型的实现就是约束模型中参数的权重。接下来我们将介绍三种不同约束权重的方法：Ridge 回归（岭回归），Lasso 回归（拉索回归）和 Elastic Net（弹性网络）。

### 岭回归

岭回归的损失函数：

$$
J(\boldsymbol{\theta}) = \text{MSE}(\boldsymbol{\theta}) + \alpha \dfrac{1}{2}\sum\limits_{i=1}^{n}(\theta_i)^2
$$

注意的是，偏置 $\theta_0$ 是没有被正则化的，累加运算开始是 $i=1$。如果定义 $w$ 作为特征的权重向量（$\theta_1$ 到 $\theta_n$，那么正则化项可以写成 $\dfrac{1}{2}{({||w||}_2)}^2$。

即，

$$
J = Loss + \dfrac{1}{2}{({||w||}_2)}^2
$$

矩阵形式的岭回归损失函数的梯度向量：

$$
\nabla_{\boldsymbol{w}} J
 = \nabla_{\boldsymbol{w}} Loss + w
$$


### 拉索回归

拉索回归的损失函数：

$$
J(\boldsymbol{\theta}) = \text{MSE}(\boldsymbol{\theta}) + \alpha \sum\limits_{i=1}^{n}\left| \theta_i \right|
$$

注意的是，偏置 $\theta_0$ 是没有被正则化的，累加运算开始是 $i=1$。如果定义 $w$ 作为特征的权重向量（$\theta_1$ 到 $\theta_n$，那么正则化项可以写成 $||w||$。

即，

$$
J = Loss + \alpha ||w||
$$

拉索回归的梯度向量：

$$
g(\boldsymbol{\theta}, J) = \nabla_{\boldsymbol{\theta}}\, \text{MSE}(\boldsymbol{\theta}) + \alpha
\begin{pmatrix}
  \operatorname{sign}(\theta_1) \\
  \operatorname{sign}(\theta_2) \\
  \vdots \\
  \operatorname{sign}(\theta_n) \\
\end{pmatrix} \quad \text{where } \operatorname{sign}(\theta_i) =
\begin{cases}
-1 & \text{if } \theta_i < 0 \\
0 & \text{if } \theta_i = 0 \\
+1 & \text{if } \theta_i > 0
\end{cases}
$$

矩阵形式的拉索回归的梯度向量：

$$
\nabla_{\boldsymbol{w}} J
 = \nabla_{\boldsymbol{w}} Loss + sign(w), \text{where } \operatorname{sign}(\theta_i) =
 \begin{cases}
 -1 & \text{if } w_{ij} < 0 \\
 0 & \text{if } w_{ij} = 0 \\
 +1 & \text{if } w_{ij} > 0
 \end{cases}
$$

### 弹性网络

弹性网络就是岭回归和拉索回归的简单混合，它的损失函数是：

$$
J(\boldsymbol{\theta}) = \text{MSE}(\boldsymbol{\theta}) + r \alpha \sum\limits_{i=1}^{n}\left| \theta_i \right| + \dfrac{1 - r}{2} \alpha \sum\limits_{i=1}^{n}{(\theta_i)^2}
$$

弹性网络损失函数的矩阵形式是：

$$
J = Loss +  r \alpha \left|| w \right||+ \dfrac{1 - r}{2} \alpha{({||w||}_2)}^2
$$
